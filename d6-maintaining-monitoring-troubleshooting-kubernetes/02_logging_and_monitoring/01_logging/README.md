
## Pods
- Check the logs for a single container pod.
```
kubectl create deployment nginx --image=nginx
PODNAME=$(kubectl get pods -l app=nginx -o jsonpath='{ .items[0].metadata.name }')
echo $PODNAME
kubectl logs $PODNAME
```

- Clean up that deployment
```
kubectl delete deployment nginx
```

- Let's create a multi-container pod that writes some information to stdout
```
kubectl apply -f multicontainer.yaml
```

- Pods a specific container in a Pod and a collection of Pods
```
PODNAME=$(kubectl get pods -l app=loggingdemo -o jsonpath='{ .items[0].metadata.name }')
echo $PODNAME
```

- Let's get the logs from the multicontainer pod, this will throw an error and ask us to define which container
```
kubectl logs $PODNAME
```

- But we need to specify which container inside the pods
```
kubectl logs $PODNAME -c container1
kubectl logs $PODNAME -c container2
```

- We can access all container logs which will dump each containers in sequence
```
kubectl logs $PODNAME --all-containers
```

- If we need to follow a log, we can do that helpful in debugging real time issues.
This works for both single and multi-container pods
```
kubectl logs $PODNAME --all-containers --follow
ctrl+c
```

- For all pods matching the selector, get all the container logs and write it to stdout and then file
```
kubectl get pods --selector app=loggingdemo
kubectl logs --selector app=loggingdemo --all-containers 
kubectl logs --selector app=loggingdemo --all-containers  > allpods.txt
```

- Also helpful is tailing the bottom of a log.
Here we're getting the last 5 log entries across all pods matching the selector.
You can do this for a single container or using a selector

```
kubectl logs --selector app=loggingdemo --all-containers --tail 5
```

## Nodes
- Get key information and status about the `kubelet`, ensure that it's active/running and check out the log. 
Also key information about it's configuration is available.
```
systemctl status kubelet.service
```

- If we want to examine it's log further, we use journalctl to access it's log from `journald -u` for which systemd unit. If using a pager, use `f` and `b` to for forward and back.
```
journalctl -u kubelet.service
```

- `journalctl` has search capabilities, but grep is likely easier
```
journalctl -u kubelet.service | grep -i ERROR
```

- Time bounding your searches can be helpful in finding issues add --no-pager for line wrapping
```
journalctl -u kubelet.service --since today --no-pager
```

## Control plane
- Get a listing of the control plane pods using a selector
```
kubectl get pods --namespace kube-system --selector tier=control-plane
```

- We can retrieve the logs for the control plane pods by using kubectl logs.
This info is coming from the API server over `kubectl`, it instructs the `kubelet` will read the log from the node and send it back to you over stdout
```
kubectl logs --namespace kube-system <kube-apiserver-pod> 
```

- But, what if your control plane is down? Go to `crictl` or to the file system.
`kubectl logs` will send the request to the local node's kubelet to read the logs from disk.
Since we're on the Control Plane Node/control plane node already we can use `crictl` for that.
```
sudo crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps
```

- Grab the log for the api server pod, paste in the CONTAINER ID using `crictl`
```
sudo crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps | grep kube-apiserver
CONTAINER_ID=$(sudo crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps | grep kube-apiserver | awk '{ print $1 }')
echo $CONTAINER_ID
sudo crictl --runtime-endpoint unix:///run/containerd/containerd.sock logs $CONTAINER_ID
```

- But, what if containerd isn't not available?
They're also available on the filesystem, here you'll find the current and the previous logs files for the containers. 
This is the same across all nodes and pods in the cluster. This also applies to user pods/containers.
These are json formmatted which is the docker/containerd logging driver default
```
sudo ls /var/log/containers
sudo tail /var/log/containers/<kube-apiserver-pod>*
```

## Events
- Show events for all objects in the cluster in the default namespace.
Look for the deployment creation and scaling operations from above.
If you don't have any events since they are only around for an hour create a deployment to generate some
```
kubectl get events 
```

- It can be easier if the data is actually sorted, `--sort-by` isn't for just events, it can be used in most output
```
kubectl get events --sort-by='.metadata.creationTimestamp'
``` 

- Create a flawed deployment
```
kubectl create deployment nginx --image ngins
```

- We can filter the list of events using field selector
```
kubectl get events --field-selector type=Warning
kubectl get events --field-selector type=Warning,reason=Failed
```

- We can also monitor the events as they happen with watch
```
kubectl get events --watch &
kubectl scale deployment loggingdemo --replicas=5
```

- break out of the watch
```
fg
ctrl+c
```

- We can look in another namespace too if needed.
```
kubectl get events --namespace kube-system
```

- These events are also available in the object as part of kubectl describe, in the events section.
```shell
kubectl describe deployment nginx
kubectl describe replicaset nginx-646c766dc9 #Update to your replicaset name
kubectl describe pods nginx
```

- Clean up our resources
```
kubectl delete -f multicontainer.yaml
kubectl delete deployment nginx
```

- But the event data is still availble from the cluster's events, even though the objects are gone.
```
kubectl get events --sort-by='.metadata.creationTimestamp'
```